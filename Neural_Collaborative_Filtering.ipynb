{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Collaborative Filtering.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python3 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanstykes/ncf/blob/master/Neural_Collaborative_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c-kbkjmoQXl8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Collaborative Filtering\n",
        "\n",
        "Neural network based collaborative filtering for recommending new products by analyzing feedbacks from users. Intended to be utilized in areas including movies, music, news, books, and products in general. In this project, I demonstrate movie recommandation using the Netflix Prize dataset, learning from both explicit and implcit feedbacks."
      ]
    },
    {
      "metadata": {
        "id": "79NnAfeOQXl-",
        "colab_type": "code",
        "colab": {},
        "outputId": "4ea1d59c-8511-4030-e84b-d24d69c9f41f"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/me/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2lCj9hDSQXmF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extracting data from files to create a user-movie matrix\n",
        "The datasets contain over 100 million ratings from 480 thousand\n",
        "randomly-chosen, anonymous Netflix customers over 17 thousand movie titles.\n",
        "\n",
        "The \"training_set\" directory contains 17770 files, one\n",
        "per movie.  The first line of each file contains the movie id followed by a\n",
        "colon.  Each subsequent line in the file corresponds to a rating from a customer\n",
        "and its date in the following format:\n",
        "\n",
        "CustomerID,Rating,Date\n",
        "\n",
        "- MovieIDs range from 1 to 17770 sequentially.\n",
        "- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
        "- Ratings are on a five star (integral) scale from 1 to 5.\n",
        "- Dates have the format YYYY-MM-DD.\n",
        "\n",
        "We ignore the dates and extract user id's and corresponding movie ratings to form a user-movie matrix."
      ]
    },
    {
      "metadata": {
        "id": "JQdsObeyQXmG",
        "colab_type": "code",
        "colab": {},
        "outputId": "9390ac0c-099f-4414-9639-a8074e679e9f"
      },
      "cell_type": "code",
      "source": [
        "tic = time.time()\n",
        "num_movies = 17770\n",
        "num_user_ids = 2649429 \n",
        "num_users = 480189\n",
        "user_movies = np.zeros((num_users, num_movies))\n",
        "user_dict = {} # user_id -> user_row\n",
        "movie_ids = []\n",
        "\n",
        "user_count = 0\n",
        "file_count = 0\n",
        "\n",
        "for filename in os.listdir(\"dataset/training_set/\"):\n",
        "    movie_file = open(\"dataset/training_set/\"+filename)\n",
        "    movie_data = movie_file.read().split(\"\\n\")\n",
        "    movie_id = int(movie_data[0].strip(\":\"))\n",
        "    movie_ids.append(movie_id)\n",
        "    #print(movie_id)\n",
        "    for i in range(1, len(movie_data) -1 ):\n",
        "        user_rating = movie_data[i].split(\",\")\n",
        "        user_id = user_rating[0]\n",
        "        rating = user_rating[1]\n",
        "        #print(user_id)\n",
        "        if user_id not in user_dict:\n",
        "            user_dict[user_id] = user_count\n",
        "            user_movies[user_count, movie_id - 1] = rating\n",
        "            user_count += 1\n",
        "        else:\n",
        "            user_movies[user_dict[user_id], movie_id - 1] = rating\n",
        "    if movie_id > 17770: \n",
        "        break\n",
        "    if file_count%1000 == 0:\n",
        "        print(\"Files loaded:\", file_count)\n",
        "    file_count+=1\n",
        "    \n",
        "toc = time.time()\n",
        "print(\"time elapsed:\",(toc - tic))\n",
        "print(\"number of users:\", user_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files loaded: 0\n",
            "Files loaded: 1000\n",
            "Files loaded: 2000\n",
            "Files loaded: 3000\n",
            "Files loaded: 4000\n",
            "Files loaded: 5000\n",
            "Files loaded: 6000\n",
            "Files loaded: 7000\n",
            "Files loaded: 8000\n",
            "Files loaded: 9000\n",
            "Files loaded: 10000\n",
            "Files loaded: 11000\n",
            "Files loaded: 12000\n",
            "Files loaded: 13000\n",
            "Files loaded: 14000\n",
            "Files loaded: 15000\n",
            "Files loaded: 16000\n",
            "Files loaded: 17000\n",
            "time elapsed: 705.0603220462799\n",
            "number of users: 480189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "juY_M_qGQXmK",
        "colab_type": "code",
        "colab": {},
        "outputId": "86f24616-c847-487e-95ae-bcce1ef10755"
      },
      "cell_type": "code",
      "source": [
        "#analyze the data\n",
        "user_id = 1488844\n",
        "movie_id = 1\n",
        "\n",
        "print(user_movies[user_dict[str(user_id)], movie_id - 1])\n",
        "j=0\n",
        "for rating in user_movies[user_dict[\"1956732\"]]:\n",
        "    if rating>0:\n",
        "        j+=1\n",
        "print(j)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kD1NIujsQXmO",
        "colab_type": "code",
        "colab": {},
        "outputId": "63f8c60a-f8e7-482a-ddb0-838857ec3dce"
      },
      "cell_type": "code",
      "source": [
        "DON'T RUN!\n",
        "#shuffle the user-movie matrix (unused)\n",
        "user_movies_train_dup = user_movies[:336132]\n",
        "np.random.shuffle(user_movies_train_dup)\n",
        "user_movies_train_dup"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "tTho3XKGQXmS",
        "colab_type": "code",
        "colab": {},
        "outputId": "e2f539cf-3711-419a-d9f4-871912bccaf6"
      },
      "cell_type": "code",
      "source": [
        "DON'T RUN!\n",
        "#test the shuffle\n",
        "user_movies_train_users = user_movies[:10000]\n",
        "nonzero_indices = np.nonzero(user_movies_train_users)\n",
        "nonzero_indices = np.array([nonzero_indices[0], nonzero_indices[1]])\n",
        "print(nonzero_indices)\n",
        "np.random.shuffle(nonzero_indices.T)\n",
        "print(nonzero_indices.shape)\n",
        "print(nonzero_indices[:, :10100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0 ...  9999  9999  9999]\n",
            " [   17    29   142 ... 17559 17626 17708]]\n",
            "(2, 8952158)\n",
            "[[ 3650  8360  1715 ...  2592  8772  4846]\n",
            " [11637  6892 10428 ... 17338  7369 14605]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BLgil36vQXmW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Processing the user-movie matrix to create an input matrix with sparse vectors as rows \n",
        "\n",
        "Each row of the input matrix will contain a concatenation of feature vectors of users and movies. Corresponding ratings are stored in a different vector."
      ]
    },
    {
      "metadata": {
        "id": "qVawV_xzQXmX",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ba3d215-8537-494a-ec09-ef82777101d1"
      },
      "cell_type": "code",
      "source": [
        "tic = time.time()\n",
        "\n",
        "user_movies_train_users = user_movies[:10000]\n",
        "#print(user_movies_train_users.shape)\n",
        "#user_movies_test = user_movies[336132:]\n",
        "\n",
        "nonzero_indices = np.nonzero(user_movies_train_users)\n",
        "nonzero_indices = np.array([nonzero_indices[0], nonzero_indices[1]]) #do the shuffle after this\n",
        "\n",
        "#shuffle\n",
        "np.random.shuffle(nonzero_indices.T)\n",
        "count_nonzero_indices = 10100\n",
        "nonzero_indices = nonzero_indices[:, :count_nonzero_indices]\n",
        "\n",
        "users = nonzero_indices[0]\n",
        "movies = nonzero_indices[1]\n",
        "\n",
        "print(\"Number of ratings:\", count_nonzero_indices)\n",
        "user_movies_train = np.zeros((count_nonzero_indices, num_users + num_movies))\n",
        "#print(user_movies_train.shape)\n",
        "ratings = np.zeros((count_nonzero_indices))\n",
        "#print(ratings.shape)\n",
        "#user_movies_log = np.empty((count_nonzero_indices, 2))\n",
        "#user_movies_train[0] = np.ones((num_users + num_movies, 1))\n",
        "\n",
        "for i in range(count_nonzero_indices):\n",
        "    rating = user_movies_train_users[users[i], movies[i]]\n",
        "    ratings[i] = rating\n",
        "    user_vector = np.expand_dims(user_movies[users[i]], axis =1)\n",
        "    #print(user_vector.shape)\n",
        "    movie_vector = np.expand_dims(user_movies[:, movies[i]] , axis=1)\n",
        "    #print(movie_vector.shape)\n",
        "    user_movies_train[i] = np.concatenate((user_vector, movie_vector), axis=0)[:,0]\n",
        "    #user_movies_log[i][0], user_movies_log[i][1] = (users[i], movies[i])\n",
        "    if(i%100 == 0):\n",
        "        print(\"completed:\", i)\n",
        "    \n",
        "print(ratings)\n",
        "toc = time.time()\n",
        "print(\"time elapsed:\",(toc - tic))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of ratings: 10100\n",
            "completed: 0\n",
            "completed: 100\n",
            "completed: 200\n",
            "completed: 300\n",
            "completed: 400\n",
            "completed: 500\n",
            "completed: 600\n",
            "completed: 700\n",
            "completed: 800\n",
            "completed: 900\n",
            "completed: 1000\n",
            "completed: 1100\n",
            "completed: 1200\n",
            "completed: 1300\n",
            "completed: 1400\n",
            "completed: 1500\n",
            "completed: 1600\n",
            "completed: 1700\n",
            "completed: 1800\n",
            "completed: 1900\n",
            "completed: 2000\n",
            "completed: 2100\n",
            "completed: 2200\n",
            "completed: 2300\n",
            "completed: 2400\n",
            "completed: 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gX33HBCrQXmb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(np.count_nonzero(user_movies_train[:,0]))\n",
        "#print(user_movies_log[1])\n",
        "nonzero_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfLLqpwJQXme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DON'T RUN!\n",
        "#for getting x to test the graph\n",
        "training_sample_size = 10\n",
        "for user_row in range(0,training_sample_size):\n",
        "    for movie_id, rating in enumerate(user_movies[user_row], start = 0):\n",
        "        X=[]\n",
        "        if rating>0:\n",
        "            X = np.expand_dims(np.concatenate((user_movies[user_row], user_movies[:, movie_id])), axis=1)\n",
        "            #print(X.shape)\n",
        "            Y = rating\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C0JlTekpQXmh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the model"
      ]
    },
    {
      "metadata": {
        "id": "2azecj3JQXmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_x, n_y, m):\n",
        "    \n",
        "    x = tf.placeholder(tf.float32, [n_x,m])#497959\n",
        "    y = tf.placeholder(tf.float32, [n_y,m])\n",
        "    \n",
        "    return x,y\n",
        "\n",
        "def initialize_parameters(n_x, n_y):\n",
        "    \n",
        "    W1 = tf.get_variable(\"W1\", [25, n_x], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
        "    W2 = tf.get_variable(\"W2\", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
        "    W3 = tf.get_variable(\"W3\", [n_y, 12], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b3 = tf.get_variable(\"b3\", [n_y,1], initializer = tf.zeros_initializer())\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def forward_propagation(x, parameters): \n",
        "    \n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "    \n",
        "    Z1 = tf.add(tf.matmul(W1, x), b1)\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)\n",
        "    y_hat = Z3 #tf.minimum(5.0, tf.maximum(0.0, Z3))\n",
        "    return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6xNe-E4QXmm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent_model(num_epochs, training_sample_size, use_train_matrix):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    x, y = create_placeholders(num_users + num_movies, 1, 1) #497959\n",
        "\n",
        "    parameters = initialize_parameters(num_users + num_movies, 1) #497959\n",
        "\n",
        "    y_hat = forward_propagation(x, parameters)\n",
        "\n",
        "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(y_hat), labels=tf.transpose(y)))\n",
        "    #print(y_hat.shape, y.shape)\n",
        "    cost = tf.losses.mean_squared_error(y, y_hat)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,\n",
        "        beta1=0.9,\n",
        "        beta2=0.999,\n",
        "        epsilon=1e-08,\n",
        "        use_locking=False,\n",
        "        name='Adam').minimize(cost)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(init)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            print(\"epoch\",epoch+1)\n",
        "            epoch_cost = 0\n",
        "            divisor = 0\n",
        "            \n",
        "            if use_train_matrix == False:\n",
        "                for user_row in range(0,training_sample_size):\n",
        "                    #print(\"user\", user_row + 1)\n",
        "                    for movie_id, rating in enumerate(user_movies[user_row]):\n",
        "                        X=[]\n",
        "                        if rating>0:\n",
        "                            user_matrix = user_movies[user_row]\n",
        "                            movie_matrix = user_movies[:, movie_id]\n",
        "                            X = np.expand_dims(np.concatenate((user_matrix, movie_matrix)), axis=1)\n",
        "                            #print(X.shape)\n",
        "                            Y = np.expand_dims(np.expand_dims(rating, axis=1),axis=1)\n",
        "                            #print(Y)\n",
        "                            _ , cost_ = sess.run([optimizer,cost], feed_dict={x:X,y:Y})\n",
        "                            epoch_cost += cost_\n",
        "                            divisor += 1\n",
        "                            break\n",
        "                            \n",
        "            elif use_train_matrix == True:\n",
        "                for index, sparse_vector in enumerate(user_movies_train):\n",
        "                    X = np.expand_dims(sparse_vector, axis=1)\n",
        "                    Y = np.expand_dims(np.expand_dims(ratings[index], axis=1), axis=1)\n",
        "                    _ , cost_ = sess.run([optimizer,cost], feed_dict={x:X,y:Y})\n",
        "                    epoch_cost += cost_\n",
        "                    divisor += 1\n",
        "                    if (index>training_sample_size):\n",
        "                        break\n",
        "                        #pass\n",
        "            \n",
        "            epoch_cost /= divisor\n",
        "            print(\"training loss:\", epoch_cost,\"\\n\")\n",
        "            \n",
        "        parameters = sess.run(parameters)\n",
        "        return parameters\n",
        "    \n",
        "def test_stochastic_gradient_descent_model(parameters, test_sample_size, show_predictions, use_train_matrix):\n",
        "    \n",
        "    cost = 0\n",
        "    test_sample_users = np.random.randint(count_nonzero_indices - training_sample_size, size=(test_sample_size,1)) + training_sample_size\n",
        "    print(test_sample_users)\n",
        "    \n",
        "    for i in range(test_sample_size):\n",
        "        \n",
        "        j = test_sample_users[i][0]\n",
        "        \n",
        "        if use_train_matrix == False:\n",
        "            \n",
        "            user_vector = np.expand_dims(user_movies[users[j]], axis =1)\n",
        "            #print(user_vector.shape)\n",
        "            movie_vector = np.expand_dims(user_movies[:, movies[j]] , axis=1)\n",
        "            #print(movie_vector.shape)\n",
        "            X_predict = np.concatenate((user_vector, movie_vector), axis=0)\n",
        "            X_predict = tf.cast(X_predict, tf.float32)\n",
        "            \n",
        "        elif use_train_matrix == True:\n",
        "            \n",
        "            X_predict = np.expand_dims(user_movies_train[j], axis=1)\n",
        "            X_predict = tf.cast(X_predict, tf.float32)\n",
        "            \n",
        "        prediction = forward_propagation(X_predict, parameters)\n",
        "        #actual_rating = user_movies_train_users[users[j], movies[j]]\n",
        "        actual_rating = ratings[j]\n",
        "        #actual_rating = expand_dims(actual_rating, axis=1)\n",
        "        \n",
        "        if show_predictions == 1:\n",
        "            sess = tf.Session()\n",
        "            print(\"prediction:\", round(sess.run(prediction)[0][0]))\n",
        "            sess.close()\n",
        "            print(\"actual rating:\", actual_rating,\"\\n\")\n",
        "        \n",
        "        cost += tf.losses.mean_squared_error(actual_rating, prediction[0,0])\n",
        "        \n",
        "    cost /= test_sample_size\n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8l7LKe7eQXmp",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b05a046-b513-41aa-b4f0-c750815b099a"
      },
      "cell_type": "code",
      "source": [
        "#train\n",
        "training_sample_size = 90\n",
        "tic = time.time()\n",
        "parameters = stochastic_gradient_descent_model(num_epochs = 20, training_sample_size = training_sample_size, use_train_matrix = True)\n",
        "toc = time.time()\n",
        "print(\"time elapsed:\",(toc - tic))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/me/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 250.7673367938753 \n",
            "\n",
            "epoch 2\n",
            "training loss: 1226.9629463429521 \n",
            "\n",
            "epoch 3\n",
            "training loss: 78.28658373067763 \n",
            "\n",
            "epoch 4\n",
            "training loss: 31.603092637229405 \n",
            "\n",
            "epoch 5\n",
            "training loss: 18.061954540416085 \n",
            "\n",
            "epoch 6\n",
            "training loss: 12.418651474472743 \n",
            "\n",
            "epoch 7\n",
            "training loss: 9.896452686276975 \n",
            "\n",
            "epoch 8\n",
            "training loss: 9.599579187624444 \n",
            "\n",
            "epoch 9\n",
            "training loss: 7.889800747633791 \n",
            "\n",
            "epoch 10\n",
            "training loss: 6.978886456989288 \n",
            "\n",
            "epoch 11\n",
            "training loss: 6.271999527572897 \n",
            "\n",
            "epoch 12\n",
            "training loss: 5.721557129388004 \n",
            "\n",
            "epoch 13\n",
            "training loss: 5.962992019298405 \n",
            "\n",
            "epoch 14\n",
            "training loss: 6.121886809753841 \n",
            "\n",
            "epoch 15\n",
            "training loss: 5.342385108542816 \n",
            "\n",
            "epoch 16\n",
            "training loss: 4.885407329256692 \n",
            "\n",
            "epoch 17\n",
            "training loss: 4.512238105420218 \n",
            "\n",
            "epoch 18\n",
            "training loss: 4.245417038778016 \n",
            "\n",
            "epoch 19\n",
            "training loss: 3.9966724224251937 \n",
            "\n",
            "epoch 20\n",
            "training loss: 3.735746755173908 \n",
            "\n",
            "time elapsed: 77.02656197547913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrHY65hNQXmt",
        "colab_type": "code",
        "colab": {},
        "outputId": "abb07ac6-a39c-4133-9587-65f20aaaad3c"
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "cost = test_stochastic_gradient_descent_model(parameters, test_sample_size = 10, show_predictions = True, use_train_matrix = True)\n",
        "sess = tf.Session()\n",
        "print(\"test loss:\", sess.run(cost))\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[90]\n",
            " [93]\n",
            " [90]\n",
            " [90]\n",
            " [98]\n",
            " [98]\n",
            " [95]\n",
            " [90]\n",
            " [98]\n",
            " [91]]\n",
            "prediction: 5.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "prediction: 3.0\n",
            "actual rating: 3.0 \n",
            "\n",
            "prediction: 5.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "prediction: 5.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "prediction: 2.0\n",
            "actual rating: 4.0 \n",
            "\n",
            "prediction: 2.0\n",
            "actual rating: 4.0 \n",
            "\n",
            "prediction: 1.0\n",
            "actual rating: 4.0 \n",
            "\n",
            "prediction: 5.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "prediction: 2.0\n",
            "actual rating: 4.0 \n",
            "\n",
            "prediction: 1.0\n",
            "actual rating: 3.0 \n",
            "\n",
            "test loss: 3.2305684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gmTfy2GPQXmx",
        "colab_type": "code",
        "colab": {},
        "outputId": "154680a5-cd9b-459d-b782-6a066fc69e68"
      },
      "cell_type": "code",
      "source": [
        "#predict\n",
        "user_id = 1025579\n",
        "movie_id = 1\n",
        "\n",
        "X_predict = np.expand_dims(np.concatenate((user_movies[user_dict[str(user_id)]], user_movies[:, movie_id - 1])), axis=1)\n",
        "X_predict = tf.cast(X_predict, tf.float32)\n",
        "prediction = forward_propagation(X_predict, parameters)\n",
        "\n",
        "sess = tf.Session()\n",
        "predicted_rating = sess.run(prediction)[0,0]\n",
        "actual_rating = user_movies[user_dict[str(user_id)], movie_id - 1]\n",
        "print(\"predicted rating:\", predicted_rating)\n",
        "print(\"actual rating:\", actual_rating)\n",
        "print(\"cost:\", sess.run(tf.losses.mean_squared_error(actual_rating, predicted_rating)))\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted rating: 0.6307114\n",
            "actual rating: 4.0\n",
            "cost: 11.352106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PXgyDgiBQXm2",
        "colab_type": "code",
        "colab": {},
        "outputId": "6b62efbc-1114-4bf4-cb4c-e6c49527e4c1"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TJto7KqaQXm6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def minibatch_gradient_descent_model(minibatch_size = 50):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    x, y = create_placeholders(num_users + num_movies, 1, minibatch_size)\n",
        "\n",
        "    parameters = initialize_parameters()\n",
        "\n",
        "    y_hat = forward_propagation(x, parameters)\n",
        "\n",
        "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(y_hat), labels=tf.transpose(y)))\n",
        "    print(y_hat.shape, y.shape)\n",
        "    cost = tf.losses.mean_squared_error(y, y_hat)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,\n",
        "        beta1=0.9,\n",
        "        beta2=0.999,\n",
        "        epsilon=1e-08,\n",
        "        use_locking=False,\n",
        "        name='Adam').minimize(cost)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(init)\n",
        "        num_epochs = 10\n",
        "        training_sample_size = 10\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(\"epoch\",epoch)\n",
        "            for user_row in range(0,training_sample_size):\n",
        "                #print(\"user\", user_row + 1)\n",
        "                for movie_id, rating in enumerate(user_movies[user_row]):\n",
        "                    X=[]\n",
        "                    if rating>0:\n",
        "                        user_matrix = np.transpose(user_movies[user_row:user_row + minibatch_size])\n",
        "                        movie_matrix = user_movies[:, movie_id:movie_id + minibatch_size]\n",
        "                        #print(movie_matrix)\n",
        "                        print(user_matrix.shape, movie_matrix.shape)\n",
        "                        X = np.concatenate((user_matrix, movie_matrix), axis=0)\n",
        "                        #print(X.shape)\n",
        "                        Y = np.expand_dims(np.expand_dims(rating, axis=1),axis=1)\n",
        "                        #print(Y)\n",
        "                        _ , cost_ = sess.run([optimizer,cost], feed_dict={x:X,y:Y})\n",
        "                        break\n",
        "            print(\"cost:\", cost_)\n",
        "        parameters = sess.run(parameters)\n",
        "        return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-pumq6DtQXm-",
        "colab_type": "code",
        "colab": {},
        "outputId": "ca2730ef-dc4f-490e-87d9-3e21c1c6dcc7"
      },
      "cell_type": "code",
      "source": [
        "parameters = minibatch_gradient_descent_model(minibatch_size = 50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50) (1, 50)\n",
            "epoch 0\n",
            "(17770, 50) (480189, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/me/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot feed value of shape (1, 1) for Tensor 'Placeholder_1:0', which has shape '(1, 50)'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-572-bfef19015b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_gradient_descent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-571-2bef4aa5986a>\u001b[0m in \u001b[0;36mminibatch_gradient_descent_model\u001b[0;34m(minibatch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m                         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0;31m#print(Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                         \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcost_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cost:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 1) for Tensor 'Placeholder_1:0', which has shape '(1, 50)'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KeGWu9XtQXnD",
        "colab_type": "code",
        "colab": {},
        "outputId": "01838420-bd8e-4b32-dc61-27afe90e147a"
      },
      "cell_type": "code",
      "source": [
        "user_id = 321111\n",
        "movie_id = 2\n",
        "\n",
        "X_test = np.expand_dims(np.concatenate((user_movies[user_dict[str(user_id)]], user_movies[:, movie_id - 1])), axis=1)\n",
        "X_test = tf.cast(X_test, tf.float32)\n",
        "print(X_test)\n",
        "prediction = forward_propagation(X_test, parameters)\n",
        "sess = tf.Session()\n",
        "print(np.math.floor(sess.run(prediction)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Cast_15:0\", shape=(497959, 1), dtype=float32)\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r9rNmEIHQXnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}